{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ToyDL Deep Learning from Scratch Overview \u00b6 ToyDL, as an education project, is aimed to make the concepts behind the deep learning as clear as possible. I do believe the key to reach the target is SIMPLE , as simple as possible. I'm trying to use little code to build this library, although it's not simple enough yet. Get Started \u00b6 Installation \u00b6 pip install toydl Documentation \u00b6 https://datahonor.com/toydl/ Related Projects \u00b6 minitorch The implementation of toydl is highly inspired by minitorch micrograd tinygrad","title":"Home"},{"location":"#overview","text":"ToyDL, as an education project, is aimed to make the concepts behind the deep learning as clear as possible. I do believe the key to reach the target is SIMPLE , as simple as possible. I'm trying to use little code to build this library, although it's not simple enough yet.","title":"Overview"},{"location":"#get-started","text":"","title":"Get Started"},{"location":"#installation","text":"pip install toydl","title":"Installation"},{"location":"#documentation","text":"https://datahonor.com/toydl/","title":"Documentation"},{"location":"#related-projects","text":"minitorch The implementation of toydl is highly inspired by minitorch micrograd tinygrad","title":"Related Projects"},{"location":"changelog/","text":"Changelog \u00b6 [Unreleased] Changed \u00b6 Poetry: Update dependencies versions Docs \u00b6 Fix mlp example: switch mode to eval before testing (#48) [0.2.0] - 2023-10-10 \u00b6 Added \u00b6 Neural network base core modules: back propagation with sgd base MLP MkDocs [0.1.0] - 2022-12-22 \u00b6 Project init","title":"Changelog"},{"location":"changelog/#changelog","text":"[Unreleased]","title":"Changelog"},{"location":"changelog/#changed","text":"Poetry: Update dependencies versions","title":"Changed"},{"location":"changelog/#docs","text":"Fix mlp example: switch mode to eval before testing (#48)","title":"Docs"},{"location":"changelog/#020-2023-10-10","text":"","title":"[0.2.0] - 2023-10-10"},{"location":"changelog/#added","text":"Neural network base core modules: back propagation with sgd base MLP MkDocs","title":"Added"},{"location":"changelog/#010-2022-12-22","text":"Project init","title":"[0.1.0] - 2022-12-22"},{"location":"api/dep_graph/","text":"Dependency Graph \u00b6","title":"Dependency Graph"},{"location":"api/dep_graph/#dependency-graph","text":"","title":"Dependency Graph"},{"location":"api/module/","text":"Module \u00b6 toydl.core.module.Module \u00b6 Module () Modules form a tree that store parameters and other submodules. They make up the basis of neural network stacks. Source code in toydl/core/module.py 13 14 15 16 def __init__ ( self ): self . _modules = {} self . _parameters = {} self . training = True add_parameter \u00b6 add_parameter ( k : str , v : Scalar ) Manually add a parameter. Useful helper for scalar parameters. Parameters: Name Type Description Default k str Local name of the parameter. required v Scalar Value for the parameter. required Returns: Type Description Newly created parameter. Source code in toydl/core/module.py 56 57 58 59 60 61 62 63 64 65 66 def add_parameter ( self , k : str , v : Scalar ): \"\"\" Manually add a parameter. Useful helper for scalar parameters. :param k: Local name of the parameter. :param v: Value for the parameter. :return parameter: Newly created parameter. \"\"\" val = Parameter ( v , k ) self . __dict__ [ \"_parameters\" ][ k ] = val return val eval \u00b6 eval () Set the mode of this module and all descendant modules to eval . Source code in toydl/core/module.py 28 29 30 31 32 def eval ( self ): \"\"\"Set the mode of this module and all descendant modules to `eval`.\"\"\" self . training = False for m in self . modules (): m . training = False modules \u00b6 modules () Return the direct child modules of this module. Source code in toydl/core/module.py 18 19 20 def modules ( self ): \"\"\"Return the direct child modules of this module.\"\"\" return self . __dict__ [ \"_modules\" ] . values () named_parameters \u00b6 named_parameters () Collect all the parameters of this module and its descendants. Returns: Type Description Contains the name and :class: Parameter of each ancestor parameter. Source code in toydl/core/module.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def named_parameters ( self ): \"\"\" Collect all the parameters of this module and its descendants. :return list of pairs: Contains the name and :class:`Parameter` of each ancestor parameter. \"\"\" named_params = [] # the module params for name , param in self . _parameters . items (): named_params . append (( name , param )) # descendants params for module_name , module in self . _modules . items (): for param_name , param in module . named_parameters (): named_params . append (( f \" { module_name } . { param_name } \" , param )) return named_params parameters \u00b6 parameters () Enumerate over all the parameters of this module and its descendants. Source code in toydl/core/module.py 51 52 53 54 def parameters ( self ): \"\"\"Enumerate over all the parameters of this module and its descendants.\"\"\" params = [ param for name , param in self . named_parameters ()] return params train \u00b6 train () Set the mode of this module and all descendant modules to train . Source code in toydl/core/module.py 22 23 24 25 26 def train ( self ): \"\"\"Set the mode of this module and all descendant modules to `train`.\"\"\" self . training = True for m in self . modules (): m . training = True toydl.core.module.Parameter \u00b6 Parameter ( value : Scalar , name : Optional [ str ] = None ) A Parameter is a special container stored in a :class: Module . It is designed to hold a :class: Variable , but we allow it to hold any value for testing. Parameters: Name Type Description Default value Scalar the value of parameter required name Optional [ str ] the name of parameter None Source code in toydl/core/module.py 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , value : Scalar , name : Optional [ str ] = None ): \"\"\" :param value: the value of parameter :param name: the name of parameter \"\"\" self . value = value self . name = name # \u8fd9\u91cc\u8bbe\u7f6e`requires_grad_`\u4e3aTrue\u53ef\u4ee5\u5c06\u5f53\u524d\u53c2\u6570\u7684\u503c,\u5373\u5bf9\u5e94\u7684Scalar instance self . value . requires_grad_ ( True ) if self . name : self . value . name = self . name update \u00b6 update ( x : Any ) Update the parameter value. Parameters: Name Type Description Default x Any the parameter's new value required Source code in toydl/core/module.py 110 111 112 113 114 115 def update ( self , x : Any ): r \"\"\"Update the parameter value. :param x: the parameter's new value \"\"\" self . value . data = x . data","title":"Module"},{"location":"api/module/#module","text":"","title":"Module"},{"location":"api/module/#toydl.core.module.Module","text":"Module () Modules form a tree that store parameters and other submodules. They make up the basis of neural network stacks. Source code in toydl/core/module.py 13 14 15 16 def __init__ ( self ): self . _modules = {} self . _parameters = {} self . training = True","title":"Module"},{"location":"api/module/#toydl.core.module.Module.add_parameter","text":"add_parameter ( k : str , v : Scalar ) Manually add a parameter. Useful helper for scalar parameters. Parameters: Name Type Description Default k str Local name of the parameter. required v Scalar Value for the parameter. required Returns: Type Description Newly created parameter. Source code in toydl/core/module.py 56 57 58 59 60 61 62 63 64 65 66 def add_parameter ( self , k : str , v : Scalar ): \"\"\" Manually add a parameter. Useful helper for scalar parameters. :param k: Local name of the parameter. :param v: Value for the parameter. :return parameter: Newly created parameter. \"\"\" val = Parameter ( v , k ) self . __dict__ [ \"_parameters\" ][ k ] = val return val","title":"add_parameter()"},{"location":"api/module/#toydl.core.module.Module.eval","text":"eval () Set the mode of this module and all descendant modules to eval . Source code in toydl/core/module.py 28 29 30 31 32 def eval ( self ): \"\"\"Set the mode of this module and all descendant modules to `eval`.\"\"\" self . training = False for m in self . modules (): m . training = False","title":"eval()"},{"location":"api/module/#toydl.core.module.Module.modules","text":"modules () Return the direct child modules of this module. Source code in toydl/core/module.py 18 19 20 def modules ( self ): \"\"\"Return the direct child modules of this module.\"\"\" return self . __dict__ [ \"_modules\" ] . values ()","title":"modules()"},{"location":"api/module/#toydl.core.module.Module.named_parameters","text":"named_parameters () Collect all the parameters of this module and its descendants. Returns: Type Description Contains the name and :class: Parameter of each ancestor parameter. Source code in toydl/core/module.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def named_parameters ( self ): \"\"\" Collect all the parameters of this module and its descendants. :return list of pairs: Contains the name and :class:`Parameter` of each ancestor parameter. \"\"\" named_params = [] # the module params for name , param in self . _parameters . items (): named_params . append (( name , param )) # descendants params for module_name , module in self . _modules . items (): for param_name , param in module . named_parameters (): named_params . append (( f \" { module_name } . { param_name } \" , param )) return named_params","title":"named_parameters()"},{"location":"api/module/#toydl.core.module.Module.parameters","text":"parameters () Enumerate over all the parameters of this module and its descendants. Source code in toydl/core/module.py 51 52 53 54 def parameters ( self ): \"\"\"Enumerate over all the parameters of this module and its descendants.\"\"\" params = [ param for name , param in self . named_parameters ()] return params","title":"parameters()"},{"location":"api/module/#toydl.core.module.Module.train","text":"train () Set the mode of this module and all descendant modules to train . Source code in toydl/core/module.py 22 23 24 25 26 def train ( self ): \"\"\"Set the mode of this module and all descendant modules to `train`.\"\"\" self . training = True for m in self . modules (): m . training = True","title":"train()"},{"location":"api/module/#toydl.core.module.Parameter","text":"Parameter ( value : Scalar , name : Optional [ str ] = None ) A Parameter is a special container stored in a :class: Module . It is designed to hold a :class: Variable , but we allow it to hold any value for testing. Parameters: Name Type Description Default value Scalar the value of parameter required name Optional [ str ] the name of parameter None Source code in toydl/core/module.py 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , value : Scalar , name : Optional [ str ] = None ): \"\"\" :param value: the value of parameter :param name: the name of parameter \"\"\" self . value = value self . name = name # \u8fd9\u91cc\u8bbe\u7f6e`requires_grad_`\u4e3aTrue\u53ef\u4ee5\u5c06\u5f53\u524d\u53c2\u6570\u7684\u503c,\u5373\u5bf9\u5e94\u7684Scalar instance self . value . requires_grad_ ( True ) if self . name : self . value . name = self . name","title":"Parameter"},{"location":"api/module/#toydl.core.module.Parameter.update","text":"update ( x : Any ) Update the parameter value. Parameters: Name Type Description Default x Any the parameter's new value required Source code in toydl/core/module.py 110 111 112 113 114 115 def update ( self , x : Any ): r \"\"\"Update the parameter value. :param x: the parameter's new value \"\"\" self . value . data = x . data","title":"update()"},{"location":"api/operator/","text":"Operator \u00b6 toydl.core.operator.add \u00b6 add ( x : float , y : float ) -> float \\(f(x, y) = x + y\\) Source code in toydl/core/operator.py 18 19 20 def add ( x : float , y : float ) -> float : \"\"\"$f(x, y) = x + y$\"\"\" return x + y toydl.core.operator.id_ \u00b6 id_ ( x : float ) -> float \\(f(x) = x\\) Source code in toydl/core/operator.py 13 14 15 def id_ ( x : float ) -> float : \"\"\"$f(x) = x$\"\"\" return x toydl.core.operator.lt \u00b6 lt ( x : float , y : float ) -> float \\(f(x, y) = x < y\\) Source code in toydl/core/operator.py 32 33 34 def lt ( x : float , y : float ) -> float : \"\"\"$f(x, y) = x < y$\"\"\" return float ( x < y ) toydl.core.operator.mul \u00b6 mul ( x : float , y : float ) -> float \\(f(x, y) = x * y\\) Source code in toydl/core/operator.py 4 5 6 def mul ( x : float , y : float ) -> float : \"\"\"$f(x, y) = x * y$\"\"\" return x * y toydl.core.operator.neg \u00b6 neg ( x : float ) -> float \\(f(x) = -x\\) Source code in toydl/core/operator.py 23 24 25 def neg ( x : float ) -> float : \"\"\"$f(x) = -x$\"\"\" return - x toydl.core.operator.relu \u00b6 relu ( x : float ) -> float f(x) = x if x is greater than 0, else 0 Source code in toydl/core/operator.py 72 73 74 75 76 def relu ( x : float ) -> float : \"\"\" f(x) = x if x is greater than 0, else 0 \"\"\" return x if x > 0 else 0 toydl.core.operator.sigmoid \u00b6 sigmoid ( x : float ) -> float \\[f(x) = \\frac{1.0}{(1.0 + e^{-x})}\\] Calculate as \\[ f(x) = \\begin{cases} \\frac{1.0}{1.0 + e^{-x}} & \\text{if } x \\geq 0 \\\\ \\frac{e^x}{1.0 + e^x} & \\text{otherwise} \\end{cases} \\] for stability. The key is to make sure the x in exp(x) is always negative to avoid exp(x) overflow. Source code in toydl/core/operator.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def sigmoid ( x : float ) -> float : r \"\"\" $$f(x) = \\frac{1.0}{(1.0 + e^{-x})}$$ Calculate as $$ f(x) = \\begin{cases} \\frac{1.0}{1.0 + e^{-x}} & \\text{if } x \\geq 0 \\\\ \\frac{e^x}{1.0 + e^x} & \\text{otherwise} \\end{cases} $$ for stability. The key is to make sure the `x` in exp(x) is always negative to avoid exp(x) overflow. \"\"\" return 1 / ( 1 + math . exp ( - x )) if x >= 0 else math . exp ( x ) / ( 1 + math . exp ( x ))","title":"Operator"},{"location":"api/operator/#operator","text":"","title":"Operator"},{"location":"api/operator/#toydl.core.operator.add","text":"add ( x : float , y : float ) -> float \\(f(x, y) = x + y\\) Source code in toydl/core/operator.py 18 19 20 def add ( x : float , y : float ) -> float : \"\"\"$f(x, y) = x + y$\"\"\" return x + y","title":"add()"},{"location":"api/operator/#toydl.core.operator.id_","text":"id_ ( x : float ) -> float \\(f(x) = x\\) Source code in toydl/core/operator.py 13 14 15 def id_ ( x : float ) -> float : \"\"\"$f(x) = x$\"\"\" return x","title":"id_()"},{"location":"api/operator/#toydl.core.operator.lt","text":"lt ( x : float , y : float ) -> float \\(f(x, y) = x < y\\) Source code in toydl/core/operator.py 32 33 34 def lt ( x : float , y : float ) -> float : \"\"\"$f(x, y) = x < y$\"\"\" return float ( x < y )","title":"lt()"},{"location":"api/operator/#toydl.core.operator.mul","text":"mul ( x : float , y : float ) -> float \\(f(x, y) = x * y\\) Source code in toydl/core/operator.py 4 5 6 def mul ( x : float , y : float ) -> float : \"\"\"$f(x, y) = x * y$\"\"\" return x * y","title":"mul()"},{"location":"api/operator/#toydl.core.operator.neg","text":"neg ( x : float ) -> float \\(f(x) = -x\\) Source code in toydl/core/operator.py 23 24 25 def neg ( x : float ) -> float : \"\"\"$f(x) = -x$\"\"\" return - x","title":"neg()"},{"location":"api/operator/#toydl.core.operator.relu","text":"relu ( x : float ) -> float f(x) = x if x is greater than 0, else 0 Source code in toydl/core/operator.py 72 73 74 75 76 def relu ( x : float ) -> float : \"\"\" f(x) = x if x is greater than 0, else 0 \"\"\" return x if x > 0 else 0","title":"relu()"},{"location":"api/operator/#toydl.core.operator.sigmoid","text":"sigmoid ( x : float ) -> float \\[f(x) = \\frac{1.0}{(1.0 + e^{-x})}\\] Calculate as \\[ f(x) = \\begin{cases} \\frac{1.0}{1.0 + e^{-x}} & \\text{if } x \\geq 0 \\\\ \\frac{e^x}{1.0 + e^x} & \\text{otherwise} \\end{cases} \\] for stability. The key is to make sure the x in exp(x) is always negative to avoid exp(x) overflow. Source code in toydl/core/operator.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def sigmoid ( x : float ) -> float : r \"\"\" $$f(x) = \\frac{1.0}{(1.0 + e^{-x})}$$ Calculate as $$ f(x) = \\begin{cases} \\frac{1.0}{1.0 + e^{-x}} & \\text{if } x \\geq 0 \\\\ \\frac{e^x}{1.0 + e^x} & \\text{otherwise} \\end{cases} $$ for stability. The key is to make sure the `x` in exp(x) is always negative to avoid exp(x) overflow. \"\"\" return 1 / ( 1 + math . exp ( - x )) if x >= 0 else math . exp ( x ) / ( 1 + math . exp ( x ))","title":"sigmoid()"},{"location":"api/optim/","text":"Optim \u00b6 toydl.core.optim.Momentum \u00b6 Momentum ( parameters : Sequence [ Parameter ], lr : float = 0.01 , momentum : float = 0.9 , ) Bases: Optimizer Stochastic Gradient Descent Optimizer Init the SGD optimizer Parameters: Name Type Description Default parameters Sequence [ Parameter ] the parameters that will be optimized required lr float learning rate 0.01 momentum float momentum coefficient 0.9 Source code in toydl/core/optim.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , parameters : Sequence [ Parameter ], lr : float = 0.01 , momentum : float = 0.9 ): \"\"\" Init the SGD optimizer :param parameters: the parameters that will be optimized :param lr: learning rate :param momentum: momentum coefficient \"\"\" super () . __init__ ( parameters ) self . lr = lr self . momentum = momentum self . parameter_delta_map : Dict [ Parameter , float ] = {} step \u00b6 step () -> None Run a sgd step to update parameter value Source code in toydl/core/optim.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def step ( self ) -> None : \"\"\" Run a sgd step to update parameter value \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : delta = ( - self . lr * p . value . derivative + self . momentum * self . parameter_delta_map . get ( p , 0 ) ) self . parameter_delta_map [ p ] = delta new_value = p . value + delta p . update ( new_value ) zero_grad \u00b6 zero_grad () -> None Clear the grad/derivative value of parameter Source code in toydl/core/optim.py 84 85 86 87 88 89 90 91 92 93 94 def zero_grad ( self ) -> None : \"\"\" Clear the grad/derivative value of parameter \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : p . value . derivative = None # Clear delta map self . parameter_delta_map = {} toydl.core.optim.Optimizer \u00b6 Optimizer ( parameters : Sequence [ Parameter ]) The Optimizer base class Source code in toydl/core/optim.py 13 14 def __init__ ( self , parameters : Sequence [ Parameter ]): self . parameters = parameters toydl.core.optim.SGD \u00b6 SGD ( parameters : Sequence [ Parameter ], lr : float = 1.0 ) Bases: Optimizer Stochastic Gradient Descent Optimizer Init the SGD optimizer Parameters: Name Type Description Default parameters Sequence [ Parameter ] the parameters that will be optimized required lr float learning rate 1.0 Source code in toydl/core/optim.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , parameters : Sequence [ Parameter ], lr : float = 1.0 ): \"\"\" Init the SGD optimizer :param parameters: the parameters that will be optimized :param lr: learning rate \"\"\" super () . __init__ ( parameters ) self . lr = lr step \u00b6 step () -> None Run a sgd step to update parameter value Source code in toydl/core/optim.py 51 52 53 54 55 56 57 58 59 60 def step ( self ) -> None : \"\"\" Run a sgd step to update parameter value \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : new_value = p . value - self . lr * p . value . derivative p . update ( new_value ) zero_grad \u00b6 zero_grad () -> None Clear the grad/derivative value of parameter Source code in toydl/core/optim.py 41 42 43 44 45 46 47 48 49 def zero_grad ( self ) -> None : \"\"\" Clear the grad/derivative value of parameter \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : p . value . derivative = None","title":"Optim"},{"location":"api/optim/#optim","text":"","title":"Optim"},{"location":"api/optim/#toydl.core.optim.Momentum","text":"Momentum ( parameters : Sequence [ Parameter ], lr : float = 0.01 , momentum : float = 0.9 , ) Bases: Optimizer Stochastic Gradient Descent Optimizer Init the SGD optimizer Parameters: Name Type Description Default parameters Sequence [ Parameter ] the parameters that will be optimized required lr float learning rate 0.01 momentum float momentum coefficient 0.9 Source code in toydl/core/optim.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , parameters : Sequence [ Parameter ], lr : float = 0.01 , momentum : float = 0.9 ): \"\"\" Init the SGD optimizer :param parameters: the parameters that will be optimized :param lr: learning rate :param momentum: momentum coefficient \"\"\" super () . __init__ ( parameters ) self . lr = lr self . momentum = momentum self . parameter_delta_map : Dict [ Parameter , float ] = {}","title":"Momentum"},{"location":"api/optim/#toydl.core.optim.Momentum.step","text":"step () -> None Run a sgd step to update parameter value Source code in toydl/core/optim.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def step ( self ) -> None : \"\"\" Run a sgd step to update parameter value \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : delta = ( - self . lr * p . value . derivative + self . momentum * self . parameter_delta_map . get ( p , 0 ) ) self . parameter_delta_map [ p ] = delta new_value = p . value + delta p . update ( new_value )","title":"step()"},{"location":"api/optim/#toydl.core.optim.Momentum.zero_grad","text":"zero_grad () -> None Clear the grad/derivative value of parameter Source code in toydl/core/optim.py 84 85 86 87 88 89 90 91 92 93 94 def zero_grad ( self ) -> None : \"\"\" Clear the grad/derivative value of parameter \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : p . value . derivative = None # Clear delta map self . parameter_delta_map = {}","title":"zero_grad()"},{"location":"api/optim/#toydl.core.optim.Optimizer","text":"Optimizer ( parameters : Sequence [ Parameter ]) The Optimizer base class Source code in toydl/core/optim.py 13 14 def __init__ ( self , parameters : Sequence [ Parameter ]): self . parameters = parameters","title":"Optimizer"},{"location":"api/optim/#toydl.core.optim.SGD","text":"SGD ( parameters : Sequence [ Parameter ], lr : float = 1.0 ) Bases: Optimizer Stochastic Gradient Descent Optimizer Init the SGD optimizer Parameters: Name Type Description Default parameters Sequence [ Parameter ] the parameters that will be optimized required lr float learning rate 1.0 Source code in toydl/core/optim.py 31 32 33 34 35 36 37 38 39 def __init__ ( self , parameters : Sequence [ Parameter ], lr : float = 1.0 ): \"\"\" Init the SGD optimizer :param parameters: the parameters that will be optimized :param lr: learning rate \"\"\" super () . __init__ ( parameters ) self . lr = lr","title":"SGD"},{"location":"api/optim/#toydl.core.optim.SGD.step","text":"step () -> None Run a sgd step to update parameter value Source code in toydl/core/optim.py 51 52 53 54 55 56 57 58 59 60 def step ( self ) -> None : \"\"\" Run a sgd step to update parameter value \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : new_value = p . value - self . lr * p . value . derivative p . update ( new_value )","title":"step()"},{"location":"api/optim/#toydl.core.optim.SGD.zero_grad","text":"zero_grad () -> None Clear the grad/derivative value of parameter Source code in toydl/core/optim.py 41 42 43 44 45 46 47 48 49 def zero_grad ( self ) -> None : \"\"\" Clear the grad/derivative value of parameter \"\"\" for p in self . parameters : if p . value is None : continue if hasattr ( p . value , \"derivative\" ) and p . value . derivative is not None : p . value . derivative = None","title":"zero_grad()"},{"location":"api/scalar/","text":"Scalar \u00b6 toydl.core.scalar.context.Context dataclass \u00b6 Context class is used by ScalarFunction to store information during the forward pass. save_for_backward \u00b6 save_for_backward ( * values : Any ) -> None Store the given values if they need to be used during backpropagation. Parameters: Name Type Description Default values Any the values that should be saved for backward () Source code in toydl/core/scalar/context.py 14 15 16 17 18 19 20 21 def save_for_backward ( self , * values : Any ) -> None : \"\"\"Store the given `values` if they need to be used during backpropagation. :param values: the values that should be saved for backward \"\"\" if self . no_grad : return self . saved_values = values toydl.core.scalar.scalar.Add \u00b6 Bases: ScalarFunction Addition function :math: f(x, y) = x + y toydl.core.scalar.scalar.EQ \u00b6 Bases: ScalarFunction Equal function :math: f(x) = 1.0 if x is equal to y else 0.0 toydl.core.scalar.scalar.Exp \u00b6 Bases: ScalarFunction Exp function toydl.core.scalar.scalar.Inv \u00b6 Bases: ScalarFunction Inverse function toydl.core.scalar.scalar.LT \u00b6 Bases: ScalarFunction Less-than function :math: f(x) = 1.0 if x is less than y else 0.0 toydl.core.scalar.scalar.Log \u00b6 Bases: ScalarFunction Log function :math: f(x) = log(x) toydl.core.scalar.scalar.Mul \u00b6 Bases: ScalarFunction Multiplication function toydl.core.scalar.scalar.Neg \u00b6 Bases: ScalarFunction Negation function toydl.core.scalar.scalar.ReLU \u00b6 Bases: ScalarFunction ReLU function toydl.core.scalar.scalar.Scalar \u00b6 Scalar ( v : float , history : ScalarHistory = ScalarHistory (), name : Optional [ str ] = None , ) A reimplementation of scalar values for auto-differentiation tracking. Scalar Variables behave as close as possible to standard Python numbers while also tracking the operations that led to the number's creation. They can only be manipulated by ScalarFunction . Source code in toydl/core/scalar/scalar.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , v : float , history : ScalarHistory = ScalarHistory (), name : Optional [ str ] = None , ): global _var_count _var_count += 1 self . _unique_id : int = _var_count self . data : float = float ( v ) self . history : ScalarHistory = history self . derivative : Optional [ float ] = None if name is not None : self . name = name else : self . name = str ( self . unique_id ) accumulate_derivative \u00b6 accumulate_derivative ( x : Any ) -> None Add x to the derivative accumulated on this variable. Should only be called during auto-differentiation on leaf variables. Parameters: Name Type Description Default x Any value to be accumulated required Source code in toydl/core/scalar/scalar.py 111 112 113 114 115 116 117 118 119 120 121 def accumulate_derivative ( self , x : Any ) -> None : \"\"\" Add `x` to the derivative accumulated on this variable. Should only be called during auto-differentiation on leaf variables. :param x: value to be accumulated \"\"\" assert self . is_leaf (), \"Only leaf variables can have derivatives.\" if self . derivative is None : self . derivative = 0.0 self . derivative += x backward \u00b6 backward ( d_output : Optional [ float ] = None ) -> None Calls autodiff to fill in the derivatives for the history of this object. Args: d_output (number, opt): starting derivative to backpropagate through the model (typically left out, and assumed to be 1.0). Source code in toydl/core/scalar/scalar.py 153 154 155 156 157 158 159 160 161 162 163 def backward ( self , d_output : Optional [ float ] = None ) -> None : \"\"\" Calls autodiff to fill in the derivatives for the history of this object. Args: d_output (number, opt): starting derivative to backpropagate through the model (typically left out, and assumed to be 1.0). \"\"\" if d_output is None : d_output = 1.0 backpropagate ( self , d_output ) is_leaf \u00b6 is_leaf () -> bool True if this variable created by the user (no last_fn ) Source code in toydl/core/scalar/scalar.py 123 124 125 def is_leaf ( self ) -> bool : \"\"\"True if this variable created by the user (no `last_fn`)\"\"\" return self . history is not None and self . history . last_fn is None requires_grad_ \u00b6 requires_grad_ ( flag : bool = True ) Set the requires_grad flag to flag on variable. Ensures that operations on this variable will trigger backpropagation. Parameters: Name Type Description Default flag bool whether to require grad True Source code in toydl/core/scalar/scalar.py 99 100 101 102 103 104 105 106 107 108 109 def requires_grad_ ( self , flag : bool = True ): \"\"\" Set the requires_grad flag to `flag` on variable. Ensures that operations on this variable will trigger backpropagation. :param flag: whether to require grad \"\"\" if flag : self . history = ScalarHistory () toydl.core.scalar.scalar.ScalarFunction \u00b6 A wrapper for a mathematical function that processes and produces Scalar variables. This is a static class and is never instantiated. We use class here to group together the forward and backward code. toydl.core.scalar.scalar.Sigmoid \u00b6 Bases: ScalarFunction Sigmoid function","title":"Scalar"},{"location":"api/scalar/#scalar","text":"","title":"Scalar"},{"location":"api/scalar/#toydl.core.scalar.context.Context","text":"Context class is used by ScalarFunction to store information during the forward pass.","title":"Context"},{"location":"api/scalar/#toydl.core.scalar.context.Context.save_for_backward","text":"save_for_backward ( * values : Any ) -> None Store the given values if they need to be used during backpropagation. Parameters: Name Type Description Default values Any the values that should be saved for backward () Source code in toydl/core/scalar/context.py 14 15 16 17 18 19 20 21 def save_for_backward ( self , * values : Any ) -> None : \"\"\"Store the given `values` if they need to be used during backpropagation. :param values: the values that should be saved for backward \"\"\" if self . no_grad : return self . saved_values = values","title":"save_for_backward()"},{"location":"api/scalar/#toydl.core.scalar.scalar.Add","text":"Bases: ScalarFunction Addition function :math: f(x, y) = x + y","title":"Add"},{"location":"api/scalar/#toydl.core.scalar.scalar.EQ","text":"Bases: ScalarFunction Equal function :math: f(x) = 1.0 if x is equal to y else 0.0","title":"EQ"},{"location":"api/scalar/#toydl.core.scalar.scalar.Exp","text":"Bases: ScalarFunction Exp function","title":"Exp"},{"location":"api/scalar/#toydl.core.scalar.scalar.Inv","text":"Bases: ScalarFunction Inverse function","title":"Inv"},{"location":"api/scalar/#toydl.core.scalar.scalar.LT","text":"Bases: ScalarFunction Less-than function :math: f(x) = 1.0 if x is less than y else 0.0","title":"LT"},{"location":"api/scalar/#toydl.core.scalar.scalar.Log","text":"Bases: ScalarFunction Log function :math: f(x) = log(x)","title":"Log"},{"location":"api/scalar/#toydl.core.scalar.scalar.Mul","text":"Bases: ScalarFunction Multiplication function","title":"Mul"},{"location":"api/scalar/#toydl.core.scalar.scalar.Neg","text":"Bases: ScalarFunction Negation function","title":"Neg"},{"location":"api/scalar/#toydl.core.scalar.scalar.ReLU","text":"Bases: ScalarFunction ReLU function","title":"ReLU"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar","text":"Scalar ( v : float , history : ScalarHistory = ScalarHistory (), name : Optional [ str ] = None , ) A reimplementation of scalar values for auto-differentiation tracking. Scalar Variables behave as close as possible to standard Python numbers while also tracking the operations that led to the number's creation. They can only be manipulated by ScalarFunction . Source code in toydl/core/scalar/scalar.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , v : float , history : ScalarHistory = ScalarHistory (), name : Optional [ str ] = None , ): global _var_count _var_count += 1 self . _unique_id : int = _var_count self . data : float = float ( v ) self . history : ScalarHistory = history self . derivative : Optional [ float ] = None if name is not None : self . name = name else : self . name = str ( self . unique_id )","title":"Scalar"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.accumulate_derivative","text":"accumulate_derivative ( x : Any ) -> None Add x to the derivative accumulated on this variable. Should only be called during auto-differentiation on leaf variables. Parameters: Name Type Description Default x Any value to be accumulated required Source code in toydl/core/scalar/scalar.py 111 112 113 114 115 116 117 118 119 120 121 def accumulate_derivative ( self , x : Any ) -> None : \"\"\" Add `x` to the derivative accumulated on this variable. Should only be called during auto-differentiation on leaf variables. :param x: value to be accumulated \"\"\" assert self . is_leaf (), \"Only leaf variables can have derivatives.\" if self . derivative is None : self . derivative = 0.0 self . derivative += x","title":"accumulate_derivative()"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.backward","text":"backward ( d_output : Optional [ float ] = None ) -> None Calls autodiff to fill in the derivatives for the history of this object. Args: d_output (number, opt): starting derivative to backpropagate through the model (typically left out, and assumed to be 1.0). Source code in toydl/core/scalar/scalar.py 153 154 155 156 157 158 159 160 161 162 163 def backward ( self , d_output : Optional [ float ] = None ) -> None : \"\"\" Calls autodiff to fill in the derivatives for the history of this object. Args: d_output (number, opt): starting derivative to backpropagate through the model (typically left out, and assumed to be 1.0). \"\"\" if d_output is None : d_output = 1.0 backpropagate ( self , d_output )","title":"backward()"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.is_leaf","text":"is_leaf () -> bool True if this variable created by the user (no last_fn ) Source code in toydl/core/scalar/scalar.py 123 124 125 def is_leaf ( self ) -> bool : \"\"\"True if this variable created by the user (no `last_fn`)\"\"\" return self . history is not None and self . history . last_fn is None","title":"is_leaf()"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.requires_grad_","text":"requires_grad_ ( flag : bool = True ) Set the requires_grad flag to flag on variable. Ensures that operations on this variable will trigger backpropagation. Parameters: Name Type Description Default flag bool whether to require grad True Source code in toydl/core/scalar/scalar.py 99 100 101 102 103 104 105 106 107 108 109 def requires_grad_ ( self , flag : bool = True ): \"\"\" Set the requires_grad flag to `flag` on variable. Ensures that operations on this variable will trigger backpropagation. :param flag: whether to require grad \"\"\" if flag : self . history = ScalarHistory ()","title":"requires_grad_()"},{"location":"api/scalar/#toydl.core.scalar.scalar.ScalarFunction","text":"A wrapper for a mathematical function that processes and produces Scalar variables. This is a static class and is never instantiated. We use class here to group together the forward and backward code.","title":"ScalarFunction"},{"location":"api/scalar/#toydl.core.scalar.scalar.Sigmoid","text":"Bases: ScalarFunction Sigmoid function","title":"Sigmoid"},{"location":"quickstart/install/","text":"Installation \u00b6 Install with Pip \u00b6 pip install toydl Check the version \u00b6 import toydl import importlib.metadata print ( importlib . metadata . version ( 'toydl' ))","title":"Installation"},{"location":"quickstart/install/#installation","text":"","title":"Installation"},{"location":"quickstart/install/#install-with-pip","text":"pip install toydl","title":"Install with Pip"},{"location":"quickstart/install/#check-the-version","text":"import toydl import importlib.metadata print ( importlib . metadata . version ( 'toydl' ))","title":"Check the version"},{"location":"quickstart/mlp/","text":"\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u673a\u7684\u4e8c\u5206\u7c7b \u00b6 \u8fd9\u91cc\u4ee5\u591a\u5c42\u611f\u77e5\u673a\u5b9e\u73b0\u6a21\u62df\u6570\u636e\u7684\u4e8c\u5206\u7c7b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd toydl \u7684\u7b80\u5355\u4f7f\u7528\u65b9\u6cd5\u3002 \u6a21\u62df\u6570\u636e\u751f\u6210 \u00b6 \u9996\u5148\u751f\u6210\u6a21\u62df\u6570\u636e\uff0c\u5e76\u5c06\u5176\u5206\u5272\u7ef4\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 def get_dataset ( n : int = 100 ) -> Tuple [ SimpleDataset , SimpleDataset ]: data = simulation_dataset . simple ( n ) training_set , test_set = data . train_test_split ( train_proportion = 0.7 ) return training_set , test_set \u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4e8c\u7ef4\u5e73\u9762\u5206\u5272\u4efb\u52a1\u7684\u6570\u636e \u914d\u7f6e\u7f51\u7edc\u7ed3\u6784 \u00b6 mlp_config = MLPConfig ( in_size = 2 , out_size = 1 , hidden_layer_size = 10 , hidden_layer_num = 2 ) mlp_model = MLPBinaryClassifyModel ( mlp_config ) \u8f93\u5165\u5c42\uff1a 2\u7ef4\u5411\u91cf \u8f93\u51fa\u5c42\uff1a 1\u7ef4\u5411\u91cf \u9690\u85cf\u5c42\uff1a\u4e24\u5c42\uff0c\u6bcf\u5c4210\u4e2a\u795e\u7ecf\u5143 \u8bad\u7ec3: SGD\u4f18\u5316\u5668 \u00b6 sgd_optimizer = SGD ( mlp_model . net . parameters (), learning_rate ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , sgd_optimizer , max_epochs = max_epochs ) mlp_model . plot_loss ( training_loss , testing_loss , title = f \"SGD: { test_result } \" , filename = \"sgd.png\" ) \u5f97\u5230\u5982\u4e0b\u7684\u635f\u5931\u66f2\u7ebf \u8bad\u7ec3: Momentum\u4f18\u5316\u5668 \u00b6 momentum = 0.5 mlp_model = MLPBinaryClassifyModel ( mlp_config ) optimizer = Momentum ( mlp_model . net . parameters (), learning_rate , momentum ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , optimizer , max_epochs = max_epochs ) \u5f97\u5230\u5982\u4e0b\u7684\u635f\u5931\u66f2\u7ebf MLP\u5b8c\u6574\u8bad\u7ec3\u6b65\u9aa4 \u00b6 class MLPBinaryClassifyModel : def __init__ ( self , mlp_config : MLPConfig ): self . net = MLPBinaryClassifyNetFactory ( mlp_config ) def forward_once ( self , x : List [ float ], y : int ) -> Tuple [ Scalar , Scalar ]: y_pred = self . net . forward ( tuple ( Scalar ( v ) for v in x )) loss = CrossEntropyLoss () . forward ( y_true = y , y_pred = y_pred ) return y_pred , loss def evaluate ( self , dateset : SimpleDataset ) -> Tuple [ float , int ]: # switch to eval mode self . net . eval () total_loss = 0.0 correct = 0 for x , y in dateset : y_pred , loss = self . forward_once ( x , y ) if y == 1 : correct += 1 if y_pred . data > 0.5 else 0 else : correct += 1 if y_pred . data < 0.5 else 0 total_loss += loss . data # switch back to train mode self . net . train () return total_loss , correct def train ( self , training_set : SimpleDataset , test_set : SimpleDataset , optimizer : Optimizer , max_epochs : int = 500 , ) -> Tuple [ List [ float ], List [ float ], str ]: training_loss , testing_loss = [], [] for epoch in range ( 1 , max_epochs + 1 ): optimizer . zero_grad () # Forward & Backward for x , y in training_set : _ , loss = self . forward_once ( x , y ) ( loss / len ( training_set )) . backward () # Update parameters optimizer . step () # Evaluation train_loss , train_correct = self . evaluate ( training_set ) test_loss , test_correct = self . evaluate ( test_set ) training_loss . append ( train_loss ) testing_loss . append ( test_loss ) if epoch % 10 == 0 or epoch == max_epochs : print ( f \"[Epoch { epoch } ]Train Loss = { train_loss } , \" f \"right( { train_correct } )/total( { len ( training_set ) } ) = { train_correct / len ( training_set ) } \\n \" f \"[Epoch { epoch } ]Test Loss = { test_loss } , \" f \"right( { test_correct } )/total( { len ( test_set ) } ) = { test_correct / len ( test_set ) } \" ) test_result = f \"right/total = { test_correct } / { len ( test_set ) } \" return training_loss , testing_loss , test_result \u5b9e\u9a8c\u5b8c\u6574\u4ee3\u7801 \u00b6 \u672c\u793a\u4f8b\u7684\u5b8c\u6574\u4ee3\u7801: example/mlp_binary.py from typing import List , Tuple import matplotlib.pyplot as plt import toydl.dataset.simulation as simulation_dataset from toydl.core.optim import SGD , Momentum , Optimizer from toydl.core.scalar.scalar import Scalar from toydl.dataset.simple import SimpleDataset from toydl.loss.cross_entropy import CrossEntropyLoss from toydl.network.mlp import MLPBinaryClassifyNetFactory , MLPConfig # --8<-- [start:model] class MLPBinaryClassifyModel : def __init__ ( self , mlp_config : MLPConfig ): self . net = MLPBinaryClassifyNetFactory ( mlp_config ) def forward_once ( self , x : List [ float ], y : int ) -> Tuple [ Scalar , Scalar ]: y_pred = self . net . forward ( tuple ( Scalar ( v ) for v in x )) loss = CrossEntropyLoss () . forward ( y_true = y , y_pred = y_pred ) return y_pred , loss def evaluate ( self , dateset : SimpleDataset ) -> Tuple [ float , int ]: # switch to eval mode self . net . eval () total_loss = 0.0 correct = 0 for x , y in dateset : y_pred , loss = self . forward_once ( x , y ) if y == 1 : correct += 1 if y_pred . data > 0.5 else 0 else : correct += 1 if y_pred . data < 0.5 else 0 total_loss += loss . data # switch back to train mode self . net . train () return total_loss , correct def train ( self , training_set : SimpleDataset , test_set : SimpleDataset , optimizer : Optimizer , max_epochs : int = 500 , ) -> Tuple [ List [ float ], List [ float ], str ]: training_loss , testing_loss = [], [] for epoch in range ( 1 , max_epochs + 1 ): optimizer . zero_grad () # Forward & Backward for x , y in training_set : _ , loss = self . forward_once ( x , y ) ( loss / len ( training_set )) . backward () # Update parameters optimizer . step () # Evaluation train_loss , train_correct = self . evaluate ( training_set ) test_loss , test_correct = self . evaluate ( test_set ) training_loss . append ( train_loss ) testing_loss . append ( test_loss ) if epoch % 10 == 0 or epoch == max_epochs : print ( f \"[Epoch { epoch } ]Train Loss = { train_loss } , \" f \"right( { train_correct } )/total( { len ( training_set ) } ) = { train_correct / len ( training_set ) } \\n \" f \"[Epoch { epoch } ]Test Loss = { test_loss } , \" f \"right( { test_correct } )/total( { len ( test_set ) } ) = { test_correct / len ( test_set ) } \" ) test_result = f \"right/total = { test_correct } / { len ( test_set ) } \" return training_loss , testing_loss , test_result # --8<-- [end:model] @staticmethod def plot_loss ( training_loss : List [ float ], testing_loss : List [ float ], title : str = \"loss plot\" , filename : str = \"loss.png\" , ): plt . clf () plt . plot ( training_loss , \"ro-\" , label = \"training loss\" ) plt . plot ( testing_loss , \"g*-\" , label = \"test loss\" ) plt . title ( title ) plt . legend () plt . tight_layout () plt . savefig ( filename , dpi = 300 ) plt . show () # --8<-- [start:gen_dateset] def get_dataset ( n : int = 100 ) -> Tuple [ SimpleDataset , SimpleDataset ]: data = simulation_dataset . simple ( n ) training_set , test_set = data . train_test_split ( train_proportion = 0.7 ) return training_set , test_set # --8<-- [end:gen_dateset] def run_sgd ( mlp_config : MLPConfig , training_set : SimpleDataset , test_set : SimpleDataset , learning_rate : float , max_epochs : int = 500 , ): mlp_model = MLPBinaryClassifyModel ( mlp_config ) sgd_optimizer = SGD ( mlp_model . net . parameters (), learning_rate ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , sgd_optimizer , max_epochs = max_epochs ) mlp_model . plot_loss ( training_loss , testing_loss , title = f \"SGD: { test_result } \" , filename = \"sgd.png\" ) def run_momentum ( mlp_config : MLPConfig , training_set : SimpleDataset , test_set : SimpleDataset , learning_rate : float , max_epochs : int = 500 , ): momentum = 0.5 mlp_model = MLPBinaryClassifyModel ( mlp_config ) optimizer = Momentum ( mlp_model . net . parameters (), learning_rate , momentum ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , optimizer , max_epochs = max_epochs ) mlp_model . plot_loss ( training_loss , testing_loss , title = f \"Momentum: { test_result } \" , filename = \"momentum.png\" , ) def run (): n = 100 training_set , test_set = get_dataset ( n ) training_set . plot ( filename = \"training_set.png\" ) mlp_config = MLPConfig ( in_size = 2 , out_size = 1 , hidden_layer_size = 10 , hidden_layer_num = 2 ) learning_rate = 0.01 max_epochs = 500 run_sgd ( mlp_config , training_set , test_set , learning_rate , max_epochs ) run_momentum ( mlp_config , training_set , test_set , learning_rate , max_epochs ) if __name__ == \"__main__\" : run ()","title":"MLP"},{"location":"quickstart/mlp/#_1","text":"\u8fd9\u91cc\u4ee5\u591a\u5c42\u611f\u77e5\u673a\u5b9e\u73b0\u6a21\u62df\u6570\u636e\u7684\u4e8c\u5206\u7c7b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd toydl \u7684\u7b80\u5355\u4f7f\u7528\u65b9\u6cd5\u3002","title":"\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u673a\u7684\u4e8c\u5206\u7c7b"},{"location":"quickstart/mlp/#_2","text":"\u9996\u5148\u751f\u6210\u6a21\u62df\u6570\u636e\uff0c\u5e76\u5c06\u5176\u5206\u5272\u7ef4\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 def get_dataset ( n : int = 100 ) -> Tuple [ SimpleDataset , SimpleDataset ]: data = simulation_dataset . simple ( n ) training_set , test_set = data . train_test_split ( train_proportion = 0.7 ) return training_set , test_set \u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4e8c\u7ef4\u5e73\u9762\u5206\u5272\u4efb\u52a1\u7684\u6570\u636e","title":"\u6a21\u62df\u6570\u636e\u751f\u6210"},{"location":"quickstart/mlp/#_3","text":"mlp_config = MLPConfig ( in_size = 2 , out_size = 1 , hidden_layer_size = 10 , hidden_layer_num = 2 ) mlp_model = MLPBinaryClassifyModel ( mlp_config ) \u8f93\u5165\u5c42\uff1a 2\u7ef4\u5411\u91cf \u8f93\u51fa\u5c42\uff1a 1\u7ef4\u5411\u91cf \u9690\u85cf\u5c42\uff1a\u4e24\u5c42\uff0c\u6bcf\u5c4210\u4e2a\u795e\u7ecf\u5143","title":"\u914d\u7f6e\u7f51\u7edc\u7ed3\u6784"},{"location":"quickstart/mlp/#sgd","text":"sgd_optimizer = SGD ( mlp_model . net . parameters (), learning_rate ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , sgd_optimizer , max_epochs = max_epochs ) mlp_model . plot_loss ( training_loss , testing_loss , title = f \"SGD: { test_result } \" , filename = \"sgd.png\" ) \u5f97\u5230\u5982\u4e0b\u7684\u635f\u5931\u66f2\u7ebf","title":"\u8bad\u7ec3: SGD\u4f18\u5316\u5668"},{"location":"quickstart/mlp/#momentum","text":"momentum = 0.5 mlp_model = MLPBinaryClassifyModel ( mlp_config ) optimizer = Momentum ( mlp_model . net . parameters (), learning_rate , momentum ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , optimizer , max_epochs = max_epochs ) \u5f97\u5230\u5982\u4e0b\u7684\u635f\u5931\u66f2\u7ebf","title":"\u8bad\u7ec3: Momentum\u4f18\u5316\u5668"},{"location":"quickstart/mlp/#mlp","text":"class MLPBinaryClassifyModel : def __init__ ( self , mlp_config : MLPConfig ): self . net = MLPBinaryClassifyNetFactory ( mlp_config ) def forward_once ( self , x : List [ float ], y : int ) -> Tuple [ Scalar , Scalar ]: y_pred = self . net . forward ( tuple ( Scalar ( v ) for v in x )) loss = CrossEntropyLoss () . forward ( y_true = y , y_pred = y_pred ) return y_pred , loss def evaluate ( self , dateset : SimpleDataset ) -> Tuple [ float , int ]: # switch to eval mode self . net . eval () total_loss = 0.0 correct = 0 for x , y in dateset : y_pred , loss = self . forward_once ( x , y ) if y == 1 : correct += 1 if y_pred . data > 0.5 else 0 else : correct += 1 if y_pred . data < 0.5 else 0 total_loss += loss . data # switch back to train mode self . net . train () return total_loss , correct def train ( self , training_set : SimpleDataset , test_set : SimpleDataset , optimizer : Optimizer , max_epochs : int = 500 , ) -> Tuple [ List [ float ], List [ float ], str ]: training_loss , testing_loss = [], [] for epoch in range ( 1 , max_epochs + 1 ): optimizer . zero_grad () # Forward & Backward for x , y in training_set : _ , loss = self . forward_once ( x , y ) ( loss / len ( training_set )) . backward () # Update parameters optimizer . step () # Evaluation train_loss , train_correct = self . evaluate ( training_set ) test_loss , test_correct = self . evaluate ( test_set ) training_loss . append ( train_loss ) testing_loss . append ( test_loss ) if epoch % 10 == 0 or epoch == max_epochs : print ( f \"[Epoch { epoch } ]Train Loss = { train_loss } , \" f \"right( { train_correct } )/total( { len ( training_set ) } ) = { train_correct / len ( training_set ) } \\n \" f \"[Epoch { epoch } ]Test Loss = { test_loss } , \" f \"right( { test_correct } )/total( { len ( test_set ) } ) = { test_correct / len ( test_set ) } \" ) test_result = f \"right/total = { test_correct } / { len ( test_set ) } \" return training_loss , testing_loss , test_result","title":"MLP\u5b8c\u6574\u8bad\u7ec3\u6b65\u9aa4"},{"location":"quickstart/mlp/#_4","text":"\u672c\u793a\u4f8b\u7684\u5b8c\u6574\u4ee3\u7801: example/mlp_binary.py from typing import List , Tuple import matplotlib.pyplot as plt import toydl.dataset.simulation as simulation_dataset from toydl.core.optim import SGD , Momentum , Optimizer from toydl.core.scalar.scalar import Scalar from toydl.dataset.simple import SimpleDataset from toydl.loss.cross_entropy import CrossEntropyLoss from toydl.network.mlp import MLPBinaryClassifyNetFactory , MLPConfig # --8<-- [start:model] class MLPBinaryClassifyModel : def __init__ ( self , mlp_config : MLPConfig ): self . net = MLPBinaryClassifyNetFactory ( mlp_config ) def forward_once ( self , x : List [ float ], y : int ) -> Tuple [ Scalar , Scalar ]: y_pred = self . net . forward ( tuple ( Scalar ( v ) for v in x )) loss = CrossEntropyLoss () . forward ( y_true = y , y_pred = y_pred ) return y_pred , loss def evaluate ( self , dateset : SimpleDataset ) -> Tuple [ float , int ]: # switch to eval mode self . net . eval () total_loss = 0.0 correct = 0 for x , y in dateset : y_pred , loss = self . forward_once ( x , y ) if y == 1 : correct += 1 if y_pred . data > 0.5 else 0 else : correct += 1 if y_pred . data < 0.5 else 0 total_loss += loss . data # switch back to train mode self . net . train () return total_loss , correct def train ( self , training_set : SimpleDataset , test_set : SimpleDataset , optimizer : Optimizer , max_epochs : int = 500 , ) -> Tuple [ List [ float ], List [ float ], str ]: training_loss , testing_loss = [], [] for epoch in range ( 1 , max_epochs + 1 ): optimizer . zero_grad () # Forward & Backward for x , y in training_set : _ , loss = self . forward_once ( x , y ) ( loss / len ( training_set )) . backward () # Update parameters optimizer . step () # Evaluation train_loss , train_correct = self . evaluate ( training_set ) test_loss , test_correct = self . evaluate ( test_set ) training_loss . append ( train_loss ) testing_loss . append ( test_loss ) if epoch % 10 == 0 or epoch == max_epochs : print ( f \"[Epoch { epoch } ]Train Loss = { train_loss } , \" f \"right( { train_correct } )/total( { len ( training_set ) } ) = { train_correct / len ( training_set ) } \\n \" f \"[Epoch { epoch } ]Test Loss = { test_loss } , \" f \"right( { test_correct } )/total( { len ( test_set ) } ) = { test_correct / len ( test_set ) } \" ) test_result = f \"right/total = { test_correct } / { len ( test_set ) } \" return training_loss , testing_loss , test_result # --8<-- [end:model] @staticmethod def plot_loss ( training_loss : List [ float ], testing_loss : List [ float ], title : str = \"loss plot\" , filename : str = \"loss.png\" , ): plt . clf () plt . plot ( training_loss , \"ro-\" , label = \"training loss\" ) plt . plot ( testing_loss , \"g*-\" , label = \"test loss\" ) plt . title ( title ) plt . legend () plt . tight_layout () plt . savefig ( filename , dpi = 300 ) plt . show () # --8<-- [start:gen_dateset] def get_dataset ( n : int = 100 ) -> Tuple [ SimpleDataset , SimpleDataset ]: data = simulation_dataset . simple ( n ) training_set , test_set = data . train_test_split ( train_proportion = 0.7 ) return training_set , test_set # --8<-- [end:gen_dateset] def run_sgd ( mlp_config : MLPConfig , training_set : SimpleDataset , test_set : SimpleDataset , learning_rate : float , max_epochs : int = 500 , ): mlp_model = MLPBinaryClassifyModel ( mlp_config ) sgd_optimizer = SGD ( mlp_model . net . parameters (), learning_rate ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , sgd_optimizer , max_epochs = max_epochs ) mlp_model . plot_loss ( training_loss , testing_loss , title = f \"SGD: { test_result } \" , filename = \"sgd.png\" ) def run_momentum ( mlp_config : MLPConfig , training_set : SimpleDataset , test_set : SimpleDataset , learning_rate : float , max_epochs : int = 500 , ): momentum = 0.5 mlp_model = MLPBinaryClassifyModel ( mlp_config ) optimizer = Momentum ( mlp_model . net . parameters (), learning_rate , momentum ) training_loss , testing_loss , test_result = mlp_model . train ( training_set , test_set , optimizer , max_epochs = max_epochs ) mlp_model . plot_loss ( training_loss , testing_loss , title = f \"Momentum: { test_result } \" , filename = \"momentum.png\" , ) def run (): n = 100 training_set , test_set = get_dataset ( n ) training_set . plot ( filename = \"training_set.png\" ) mlp_config = MLPConfig ( in_size = 2 , out_size = 1 , hidden_layer_size = 10 , hidden_layer_num = 2 ) learning_rate = 0.01 max_epochs = 500 run_sgd ( mlp_config , training_set , test_set , learning_rate , max_epochs ) run_momentum ( mlp_config , training_set , test_set , learning_rate , max_epochs ) if __name__ == \"__main__\" : run ()","title":"\u5b9e\u9a8c\u5b8c\u6574\u4ee3\u7801"}]}