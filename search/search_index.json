{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ToyDL: Deep Learning from Scratch","text":"<p>ToyDL, as an education project, is aimed to make the concepts behind the deep learning as clear as possible. I do believe the key to reach the target is SIMPLE, as simple as possible. I'm trying to use little code to build this library, although it's not simple enough yet.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install toydl\n</code></pre>"},{"location":"#related-projects","title":"Related Projects","text":"<ul> <li>minitorch: The implementation of <code>toydl</code> is highly inspired by minitorch</li> <li>micrograd: https://github.com/karpathy/micrograd</li> <li>tinygrad: https://github.com/tinygrad/tinygrad</li> <li>nanograd: https://github.com/e3ntity/nanograd</li> </ul>"},{"location":"api/dep_graph/","title":"Dependency Graph","text":""},{"location":"api/module/","title":"Module","text":""},{"location":"api/module/#toydl.core.module.Module","title":"toydl.core.module.Module","text":"<pre><code>Module()\n</code></pre> <p>Modules form a tree that stores parameters and other submodules. They make up the basis of neural network stacks.</p> Source code in <code>toydl/core/module.py</code> <pre><code>def __init__(self):\n    self._modules: dict[str, \"Module\"] = {}\n    self._parameters: dict[str, Parameter] = {}\n    self.training = True\n</code></pre>"},{"location":"api/module/#toydl.core.module.Module.add_parameter","title":"add_parameter","text":"<pre><code>add_parameter(k: str, v: Scalar)\n</code></pre> <p>Manually add a parameter. Useful helper for scalar parameters.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>str</code> <p>Local name of the parameter.</p> required <code>v</code> <code>Scalar</code> <p>Value for the parameter.</p> required <p>Returns:</p> Type Description <p>Newly created parameter.</p> Source code in <code>toydl/core/module.py</code> <pre><code>def add_parameter(self, k: str, v: Scalar):\n    \"\"\"\n    Manually add a parameter. Useful helper for scalar parameters.\n\n    :param k: Local name of the parameter.\n    :param v: Value for the parameter.\n    :return parameter: Newly created parameter.\n    \"\"\"\n    val = Parameter(v, k)\n    self.__dict__[\"_parameters\"][k] = val\n    return val\n</code></pre>"},{"location":"api/module/#toydl.core.module.Module.eval","title":"eval","text":"<pre><code>eval()\n</code></pre> <p>Set the mode of this module and all descendant modules to <code>eval</code>.</p> Source code in <code>toydl/core/module.py</code> <pre><code>def eval(self):\n    \"\"\"Set the mode of this module and all descendant modules to `eval`.\"\"\"\n    self.training = False\n    for m in self.modules():\n        m.training = False\n</code></pre>"},{"location":"api/module/#toydl.core.module.Module.modules","title":"modules","text":"<pre><code>modules()\n</code></pre> <p>Return the direct child modules of this module.</p> Source code in <code>toydl/core/module.py</code> <pre><code>def modules(self):\n    \"\"\"Return the direct child modules of this module.\"\"\"\n    return self.__dict__[\"_modules\"].values()\n</code></pre>"},{"location":"api/module/#toydl.core.module.Module.named_parameters","title":"named_parameters","text":"<pre><code>named_parameters() -&gt; list[tuple[str, Parameter]]\n</code></pre> <p>Collect all the parameters of this module and its descendants.</p> <p>Returns:</p> Type Description <code>list[tuple[str, Parameter]]</code> <p>Contains the name and :class:<code>Parameter</code> of each ancestor parameter.</p> Source code in <code>toydl/core/module.py</code> <pre><code>def named_parameters(self) -&gt; list[tuple[str, Parameter]]:\n    \"\"\"\n    Collect all the parameters of this module and its descendants.\n\n\n    :return list of pairs: Contains the name and :class:`Parameter` of each ancestor parameter.\n    \"\"\"\n    named_params: list[tuple[str, Parameter]] = []\n    # the module params\n    for name, param in self._parameters.items():\n        named_params.append((name, param))\n    # descendants params\n    for module_name, module in self._modules.items():\n        for param_name, param in module.named_parameters():\n            named_params.append((f\"{module_name}.{param_name}\", param))\n    return named_params\n</code></pre>"},{"location":"api/module/#toydl.core.module.Module.parameters","title":"parameters","text":"<pre><code>parameters()\n</code></pre> <p>Enumerate over all the parameters of this module and its descendants.</p> Source code in <code>toydl/core/module.py</code> <pre><code>def parameters(self):\n    \"\"\"Enumerate over all the parameters of this module and its descendants.\"\"\"\n    params = [param for _, param in self.named_parameters()]\n    return params\n</code></pre>"},{"location":"api/module/#toydl.core.module.Module.train","title":"train","text":"<pre><code>train()\n</code></pre> <p>Set the mode of this module and all descendant modules to <code>train</code>.</p> Source code in <code>toydl/core/module.py</code> <pre><code>def train(self):\n    \"\"\"Set the mode of this module and all descendant modules to `train`.\"\"\"\n    self.training = True\n    for m in self.modules():\n        m.training = True\n</code></pre>"},{"location":"api/module/#toydl.core.module.Parameter","title":"toydl.core.module.Parameter","text":"<pre><code>Parameter(value: Scalar, name: str | None = None)\n</code></pre> <p>A Parameter is a special container stored in a :class:<code>Module</code>.</p> <p>It is designed to hold a :class:<code>Variable</code>, but we allow it to hold any value for testing.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Scalar</code> <p>the value of parameter</p> required <code>name</code> <code>str | None</code> <p>the name of parameter</p> <code>None</code> Source code in <code>toydl/core/module.py</code> <pre><code>def __init__(self, value: Scalar, name: str | None = None):\n    \"\"\"\n    :param value: the value of parameter\n    :param name: the name of parameter\n    \"\"\"\n    self.value = value\n    self.name = name\n    # Set `requires_grad_` to True, indicating that these parameters are trainable (can accumulate gradients)\n    self.value.requires_grad_(True)\n    if self.name:\n        self.value.name = self.name\n</code></pre>"},{"location":"api/module/#toydl.core.module.Parameter.update","title":"update","text":"<pre><code>update(x: Scalar) -&gt; None\n</code></pre> <p>Update the parameter value.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Scalar</code> <p>the parameter's new value</p> required Source code in <code>toydl/core/module.py</code> <pre><code>def update(self, x: Scalar) -&gt; None:\n    r\"\"\"Update the parameter value.\n\n    :param x: the parameter's new value\n    \"\"\"\n    self.value.data = x.data\n</code></pre>"},{"location":"api/operator/","title":"Operator","text":""},{"location":"api/operator/#toydl.core.operator.add","title":"toydl.core.operator.add","text":"<pre><code>add(x: float, y: float) -&gt; float\n</code></pre> <p>\\(f(x, y) = x + y\\)</p> Source code in <code>toydl/core/operator.py</code> <pre><code>def add(x: float, y: float) -&gt; float:\n    \"\"\"$f(x, y) = x + y$\"\"\"\n    return x + y\n</code></pre>"},{"location":"api/operator/#toydl.core.operator.id_","title":"toydl.core.operator.id_","text":"<pre><code>id_(x: float) -&gt; float\n</code></pre> <p>\\(f(x) = x\\)</p> Source code in <code>toydl/core/operator.py</code> <pre><code>def id_(x: float) -&gt; float:\n    \"\"\"$f(x) = x$\"\"\"\n    return x\n</code></pre>"},{"location":"api/operator/#toydl.core.operator.lt","title":"toydl.core.operator.lt","text":"<pre><code>lt(x: float, y: float) -&gt; float\n</code></pre> <p>\\(f(x, y) = x &lt; y\\)</p> Source code in <code>toydl/core/operator.py</code> <pre><code>def lt(x: float, y: float) -&gt; float:\n    \"\"\"$f(x, y) = x &lt; y$\"\"\"\n    return float(x &lt; y)\n</code></pre>"},{"location":"api/operator/#toydl.core.operator.mul","title":"toydl.core.operator.mul","text":"<pre><code>mul(x: float, y: float) -&gt; float\n</code></pre> <p>\\(f(x, y) = x * y\\)</p> Source code in <code>toydl/core/operator.py</code> <pre><code>def mul(x: float, y: float) -&gt; float:\n    \"\"\"$f(x, y) = x * y$\"\"\"\n    return x * y\n</code></pre>"},{"location":"api/operator/#toydl.core.operator.neg","title":"toydl.core.operator.neg","text":"<pre><code>neg(x: float) -&gt; float\n</code></pre> <p>\\(f(x) = -x\\)</p> Source code in <code>toydl/core/operator.py</code> <pre><code>def neg(x: float) -&gt; float:\n    \"\"\"$f(x) = -x$\"\"\"\n    return -x\n</code></pre>"},{"location":"api/operator/#toydl.core.operator.relu","title":"toydl.core.operator.relu","text":"<pre><code>relu(x: float) -&gt; float\n</code></pre> <p>f(x) = x if x is greater than 0, else 0</p> Source code in <code>toydl/core/operator.py</code> <pre><code>def relu(x: float) -&gt; float:\n    \"\"\"\n    f(x) = x if x is greater than 0, else 0\n    \"\"\"\n    return x if x &gt; 0 else 0\n</code></pre>"},{"location":"api/operator/#toydl.core.operator.sigmoid","title":"toydl.core.operator.sigmoid","text":"<pre><code>sigmoid(x: float) -&gt; float\n</code></pre> \\[f(x) =  \\frac{1.0}{(1.0 + e^{-x})}\\] <p>Calculate as</p> \\[ f(x) = \\begin{cases} \\frac{1.0}{1.0 + e^{-x}} &amp; \\text{if } x \\geq 0 \\\\ \\frac{e^x}{1.0 + e^x} &amp; \\text{otherwise} \\end{cases} \\] <p>for stability. The key is to make sure the <code>x</code> in exp(x) is always negative to avoid exp(x) overflow.</p> Source code in <code>toydl/core/operator.py</code> <pre><code>def sigmoid(x: float) -&gt; float:\n    r\"\"\"\n    $$f(x) =  \\frac{1.0}{(1.0 + e^{-x})}$$\n\n    Calculate as\n\n    $$\n    f(x) = \\begin{cases}\n    \\frac{1.0}{1.0 + e^{-x}} &amp; \\text{if } x \\geq 0 \\\\\n    \\frac{e^x}{1.0 + e^x} &amp; \\text{otherwise}\n    \\end{cases}\n    $$\n\n    for stability.\n    The key is to make sure the `x` in exp(x) is always negative to avoid exp(x) overflow.\n    \"\"\"\n    return 1 / (1 + math.exp(-x)) if x &gt;= 0 else math.exp(x) / (1 + math.exp(x))\n</code></pre>"},{"location":"api/optim/","title":"Optim","text":""},{"location":"api/optim/#toydl.core.optim.Momentum","title":"toydl.core.optim.Momentum","text":"<pre><code>Momentum(parameters: Sequence[Parameter], lr: float = 0.01, momentum: float = 0.9)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Momentum Optimizer</p> <p>Init the Momentum optimizer</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Sequence[Parameter]</code> <p>the parameters that will be optimized</p> required <code>lr</code> <code>float</code> <p>learning rate</p> <code>0.01</code> <code>momentum</code> <code>float</code> <p>momentum coefficient</p> <code>0.9</code> Source code in <code>toydl/core/optim/_momentum.py</code> <pre><code>def __init__(\n    self, parameters: Sequence[Parameter], lr: float = 0.01, momentum: float = 0.9\n):\n    \"\"\"\n    Init the Momentum optimizer\n\n    :param parameters: the parameters that will be optimized\n    :param lr: learning rate\n    :param momentum: momentum coefficient\n    \"\"\"\n    super().__init__(parameters)\n    self.lr = lr\n    self.momentum = momentum\n    self.parameter_delta_map: dict[Parameter, float] = {}\n</code></pre>"},{"location":"api/optim/#toydl.core.optim.Momentum.step","title":"step","text":"<pre><code>step() -&gt; None\n</code></pre> <p>Run a momentum step to update parameter value</p> Source code in <code>toydl/core/optim/_momentum.py</code> <pre><code>def step(self) -&gt; None:\n    \"\"\"\n    Run a momentum step to update parameter value\n    \"\"\"\n    for p in self.parameters:\n        if hasattr(p.value, \"derivative\") and p.value.derivative is not None:\n            delta = (\n                -self.lr * p.value.derivative\n                + self.momentum * self.parameter_delta_map.get(p, 0)\n            )\n            self.parameter_delta_map[p] = delta\n            new_value = p.value + delta\n            p.update(new_value)\n</code></pre>"},{"location":"api/optim/#toydl.core.optim.Momentum.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad() -&gt; None\n</code></pre> <p>Clear the grad/derivative value of parameter</p> Source code in <code>toydl/core/optim/_momentum.py</code> <pre><code>def zero_grad(self) -&gt; None:\n    \"\"\"\n    Clear the grad/derivative value of parameter\n    \"\"\"\n    for p in self.parameters:\n        if hasattr(p.value, \"derivative\") and p.value.derivative is not None:\n            p.value.derivative = None\n</code></pre>"},{"location":"api/optim/#toydl.core.optim.Optimizer","title":"toydl.core.optim.Optimizer","text":"<pre><code>Optimizer(parameters: Sequence[Parameter])\n</code></pre> <p>The Optimizer base class</p> Source code in <code>toydl/core/optim/_base.py</code> <pre><code>def __init__(self, parameters: Sequence[Parameter]):\n    self.parameters = parameters\n</code></pre>"},{"location":"api/optim/#toydl.core.optim.SGD","title":"toydl.core.optim.SGD","text":"<pre><code>SGD(parameters: Sequence[Parameter], lr: float = 1.0)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Stochastic Gradient Descent Optimizer</p> <p>Init the SGD optimizer</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Sequence[Parameter]</code> <p>the parameters that will be optimized</p> required <code>lr</code> <code>float</code> <p>learning rate</p> <code>1.0</code> Source code in <code>toydl/core/optim/_sgd.py</code> <pre><code>def __init__(self, parameters: Sequence[Parameter], lr: float = 1.0):\n    \"\"\"\n    Init the SGD optimizer\n\n    :param parameters: the parameters that will be optimized\n    :param lr: learning rate\n    \"\"\"\n    super().__init__(parameters)\n    self.lr = lr\n</code></pre>"},{"location":"api/optim/#toydl.core.optim.SGD.step","title":"step","text":"<pre><code>step() -&gt; None\n</code></pre> <p>Run a sgd step to update parameter value</p> Source code in <code>toydl/core/optim/_sgd.py</code> <pre><code>def step(self) -&gt; None:\n    \"\"\"\n    Run a sgd step to update parameter value\n    \"\"\"\n    for p in self.parameters:\n        if hasattr(p.value, \"derivative\") and p.value.derivative is not None:\n            new_value = p.value - self.lr * p.value.derivative\n            p.update(new_value)\n</code></pre>"},{"location":"api/optim/#toydl.core.optim.SGD.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad() -&gt; None\n</code></pre> <p>Clear the grad/derivative value of parameter</p> Source code in <code>toydl/core/optim/_sgd.py</code> <pre><code>def zero_grad(self) -&gt; None:\n    \"\"\"\n    Clear the grad/derivative value of parameter\n    \"\"\"\n    for p in self.parameters:\n        if hasattr(p.value, \"derivative\") and p.value.derivative is not None:\n            p.value.derivative = None\n</code></pre>"},{"location":"api/scalar/","title":"Scalar","text":""},{"location":"api/scalar/#toydl.core.scalar.context.Context","title":"toydl.core.scalar.context.Context  <code>dataclass</code>","text":"<pre><code>Context(no_grad: bool = False, saved_values: tuple[Any, ...] = ())\n</code></pre> <p>Context class is used by <code>ScalarFunction</code> to store information during the forward pass.</p>"},{"location":"api/scalar/#toydl.core.scalar.context.Context.save_for_backward","title":"save_for_backward","text":"<pre><code>save_for_backward(*values: Any) -&gt; None\n</code></pre> <p>Store the given <code>values</code> if they need to be used during backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Any</code> <p>The values that should be saved for backward</p> <code>()</code> Source code in <code>toydl/core/scalar/context.py</code> <pre><code>def save_for_backward(self, *values: Any) -&gt; None:\n    \"\"\"Store the given `values` if they need to be used during backpropagation.\n\n    :param values: The values that should be saved for backward\n    \"\"\"\n    if self.no_grad:\n        return\n    self.saved_values = values\n</code></pre>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Add","title":"toydl.core.scalar.scalar.Add","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Addition function :math:<code>f(x, y) = x + y</code></p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.EQ","title":"toydl.core.scalar.scalar.EQ","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Equal function :math:<code>f(x) =</code> 1.0 if x is equal to y else 0.0</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Exp","title":"toydl.core.scalar.scalar.Exp","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Exp function</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Inv","title":"toydl.core.scalar.scalar.Inv","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Inverse function</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.LT","title":"toydl.core.scalar.scalar.LT","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Less-than function :math:<code>f(x) =</code> 1.0 if x is less than y else 0.0</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Log","title":"toydl.core.scalar.scalar.Log","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Log function :math:<code>f(x) = log(x)</code></p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Mul","title":"toydl.core.scalar.scalar.Mul","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Multiplication function</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Neg","title":"toydl.core.scalar.scalar.Neg","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Negation function</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.ReLU","title":"toydl.core.scalar.scalar.ReLU","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>ReLU function</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar","title":"toydl.core.scalar.scalar.Scalar","text":"<pre><code>Scalar(v: float, history: ScalarHistory | None = None, name: Optional[str] = None)\n</code></pre> <p>A reimplementation of scalar values for auto-differentiation tracking. Scalar Variables behave as close as possible to standard Python numbers while also tracking the operations that led to the number's creation. They can only be manipulated by <code>ScalarFunction</code>.</p> Source code in <code>toydl/core/scalar/scalar.py</code> <pre><code>def __init__(\n    self,\n    v: float,\n    history: ScalarHistory | None = None,\n    name: Optional[str] = None,\n):\n    global _var_count\n    _var_count += 1\n    self._unique_id: int = _var_count\n    self.data: float = float(v)\n    self.history = history\n    self.derivative: Optional[float] = None\n    if name is not None:\n        self.name = name\n    else:\n        self.name = str(self.unique_id)\n</code></pre>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.accumulate_derivative","title":"accumulate_derivative","text":"<pre><code>accumulate_derivative(x: Any) -&gt; None\n</code></pre> <p>Add <code>x</code> to the derivative accumulated on this variable. Should only be called during auto-differentiation on leaf variables.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>value to be accumulated</p> required Source code in <code>toydl/core/scalar/scalar.py</code> <pre><code>def accumulate_derivative(self, x: Any) -&gt; None:\n    \"\"\"\n    Add `x` to the derivative accumulated on this variable.\n    Should only be called during auto-differentiation on leaf variables.\n\n    :param x: value to be accumulated\n    \"\"\"\n    assert self.is_leaf(), \"Only leaf variables can have derivatives.\"\n    if self.derivative is None:\n        self.derivative = 0.0\n    self.derivative += x\n</code></pre>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.backward","title":"backward","text":"<pre><code>backward(d_output: Optional[float] = None) -&gt; None\n</code></pre> <p>Calls autodiff to fill in the derivatives for the history of this object.</p> <p>Args:     d_output (number, opt): starting derivative to backpropagate through the model                            (typically left out, and assumed to be 1.0).</p> Source code in <code>toydl/core/scalar/scalar.py</code> <pre><code>def backward(self, d_output: Optional[float] = None) -&gt; None:\n    \"\"\"\n    Calls autodiff to fill in the derivatives for the history of this object.\n\n    Args:\n        d_output (number, opt): starting derivative to backpropagate through the model\n                               (typically left out, and assumed to be 1.0).\n    \"\"\"\n    if d_output is None:\n        d_output = 1.0\n    backpropagate(self, d_output)\n</code></pre>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.is_leaf","title":"is_leaf","text":"<pre><code>is_leaf() -&gt; bool\n</code></pre> <p>True if this variable created by the user (no <code>last_fn</code>)</p> Source code in <code>toydl/core/scalar/scalar.py</code> <pre><code>def is_leaf(self) -&gt; bool:\n    \"\"\"True if this variable created by the user (no `last_fn`)\"\"\"\n    return self.history is not None and self.history.last_fn is None\n</code></pre>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Scalar.requires_grad_","title":"requires_grad_","text":"<pre><code>requires_grad_(flag: bool = True)\n</code></pre> <p>Set the requires_grad flag to <code>flag</code> on variable.</p> <p>Ensures that operations on this variable will trigger backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>bool</code> <p>whether to require grad</p> <code>True</code> Source code in <code>toydl/core/scalar/scalar.py</code> <pre><code>def requires_grad_(self, flag: bool = True):\n    \"\"\"\n    Set the requires_grad flag to `flag` on variable.\n\n    Ensures that operations on this variable will trigger\n    backpropagation.\n\n    :param flag: whether to require grad\n    \"\"\"\n    if flag:\n        self.history = ScalarHistory()\n</code></pre>"},{"location":"api/scalar/#toydl.core.scalar.scalar.ScalarFunction","title":"toydl.core.scalar.scalar.ScalarFunction","text":"<p>               Bases: <code>ABC</code></p> <p>A wrapper for a mathematical function that processes and produces Scalar variables.</p> <p>This is a static class and is never instantiated. We use <code>class</code> here to group together the <code>forward</code> and <code>backward</code> code.</p>"},{"location":"api/scalar/#toydl.core.scalar.scalar.Sigmoid","title":"toydl.core.scalar.scalar.Sigmoid","text":"<p>               Bases: <code>ScalarFunction</code></p> <p>Sigmoid function</p>"},{"location":"quickstart/install/","title":"Installation","text":""},{"location":"quickstart/install/#install-with-pip","title":"Install with Pip","text":"<p><code>pip install toydl</code></p>"},{"location":"quickstart/install/#check-the-version","title":"Check the version","text":"<pre><code>import toydl\nimport importlib.metadata\n\nprint(importlib.metadata.version('toydl'))\n</code></pre>"},{"location":"quickstart/mlp/","title":"\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u673a\u7684\u4e8c\u5206\u7c7b","text":"<p>\u8fd9\u91cc\u4ee5\u591a\u5c42\u611f\u77e5\u673a\u5b9e\u73b0\u6a21\u62df\u6570\u636e\u7684\u4e8c\u5206\u7c7b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd<code>toydl</code>\u7684\u7b80\u5355\u4f7f\u7528\u65b9\u6cd5\u3002</p>"},{"location":"quickstart/mlp/#_2","title":"\u6a21\u62df\u6570\u636e\u751f\u6210","text":"<p>\u9996\u5148\u751f\u6210\u6a21\u62df\u6570\u636e\uff0c\u5e76\u5c06\u5176\u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a</p> <pre><code>def get_dataset(n: int = 100) -&gt; Tuple[SimpleDataset, SimpleDataset]:\n    data = simulation_dataset.diag(n)\n    training_set, test_set = data.train_test_split(train_proportion=0.7)\n\n    return training_set, test_set\n</code></pre> <p>\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4e8c\u7ef4\u5e73\u9762\u5206\u5272\u4efb\u52a1\u7684\u6570\u636e\uff1a</p> <p> </p>"},{"location":"quickstart/mlp/#_3","title":"\u914d\u7f6e\u7f51\u7edc\u7ed3\u6784","text":"<pre><code>mlp_config = MLPConfig(\n    in_size=2, out_size=1, hidden_layer_size=10, hidden_layer_num=2\n)\n\nmlp_model = MLPBinaryClassifyModel(mlp_config)\n</code></pre> <ul> <li>\u8f93\u5165\u5c42\uff1a2\u7ef4\u5411\u91cf</li> <li>\u8f93\u51fa\u5c42\uff1a1\u7ef4\u5411\u91cf</li> <li>\u9690\u85cf\u5c42\uff1a\u4e24\u5c42\uff0c\u6bcf\u5c4210\u4e2a\u795e\u7ecf\u5143</li> </ul>"},{"location":"quickstart/mlp/#_4","title":"\u8bad\u7ec3\u4e0d\u540c\u4f18\u5316\u5668\u6a21\u578b","text":""},{"location":"quickstart/mlp/#sgd","title":"SGD\u4f18\u5316\u5668","text":"<pre><code>def run_sgd(\n    mlp_config: MLPConfig,\n    training_set: SimpleDataset,\n    test_set: SimpleDataset,\n    learning_rate: float,\n    max_epochs: int = 500,\n):\n    mlp_model = MLPBinaryClassifyModel(mlp_config)\n    sgd_optimizer = SGD(mlp_model.net.parameters(), learning_rate)\n    training_loss, testing_loss, test_result = mlp_model.train(\n        training_set, test_set, sgd_optimizer, max_epochs=max_epochs\n    )\n    return training_loss, testing_loss, test_result\n</code></pre>"},{"location":"quickstart/mlp/#momentum","title":"Momentum\u4f18\u5316\u5668","text":"<pre><code>def run_momentum(\n    mlp_config: MLPConfig,\n    training_set: SimpleDataset,\n    test_set: SimpleDataset,\n    learning_rate: float,\n    max_epochs: int = 500,\n):\n    momentum = 0.5\n    mlp_model = MLPBinaryClassifyModel(mlp_config)\n    optimizer = Momentum(mlp_model.net.parameters(), learning_rate, momentum)\n    training_loss, testing_loss, test_result = mlp_model.train(\n        training_set, test_set, optimizer, max_epochs=max_epochs\n    )\n    return training_loss, testing_loss, test_result\n</code></pre>"},{"location":"quickstart/mlp/#_5","title":"\u4f18\u5316\u5668\u6027\u80fd\u5bf9\u6bd4","text":"<p>\u901a\u8fc7\u5728\u4e00\u4e2a\u56fe\u8868\u4e2d\u540c\u65f6\u5c55\u793a\u4e0d\u540c\u4f18\u5316\u5668\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u635f\u5931\u66f2\u7ebf\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u89c2\u5730\u6bd4\u8f83\u5b83\u4eec\u7684\u6027\u80fd\u3002<code>plot_multiple_optimizers</code>\u51fd\u6570\u53ef\u4ee5\u63a5\u6536\u591a\u4e2a\u4f18\u5316\u5668\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u5c06\u5b83\u4eec\u7ed8\u5236\u5728\u540c\u4e00\u5f20\u56fe\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002</p>"},{"location":"quickstart/mlp/#_6","title":"\u8fd0\u884c\u4f18\u5316\u5668\u6bd4\u8f83","text":"<pre><code>def run():\n    n = 100\n    training_set, test_set = get_dataset(n)\n\n    mlp_config = MLPConfig(\n        in_size=2, out_size=1, hidden_layer_size=10, hidden_layer_num=2\n    )\n\n    learning_rate = 0.01\n    max_epochs = 500\n\n    # \u8bad\u7ec3SGD\u6a21\u578b\n    sgd_training_loss, sgd_testing_loss, sgd_test_result = run_sgd(\n        mlp_config, training_set, test_set, learning_rate, max_epochs\n    )\n\n    # \u8bad\u7ec3Momentum\u6a21\u578b\n    momentum_training_loss, momentum_testing_loss, momentum_result = run_momentum(\n        mlp_config, training_set, test_set, learning_rate, max_epochs\n    )\n\n    # \u6bd4\u8f83\u4f18\u5316\u5668\u6027\u80fd\n    optimizer_results = {\n        \"SGD\": (sgd_training_loss, sgd_testing_loss, sgd_test_result),\n        \"Momentum\": (momentum_training_loss, momentum_testing_loss, momentum_result)\n    }\n    plot_multiple_optimizers(optimizer_results, title=\"\u4f18\u5316\u5668\u6027\u80fd\u5bf9\u6bd4\")\n</code></pre> <p>\u8fd0\u884c\u4e0a\u8ff0\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5f97\u5230\u5982\u4e0b\u7684\u4f18\u5316\u5668\u6027\u80fd\u5bf9\u6bd4\u56fe\uff1a</p> <p> </p>"},{"location":"quickstart/mlp/#mlp","title":"MLP\u5b8c\u6574\u8bad\u7ec3\u6b65\u9aa4","text":"<pre><code>class MLPBinaryClassifyModel:\n    def __init__(self, mlp_config: MLPConfig):\n        self.net = MLPBinaryClassifyNetFactory(mlp_config)\n\n    def forward_once(self, x: list[float], y: int) -&gt; tuple[Scalar, Scalar]:\n        y_pred = self.net.forward(list(Scalar(v) for v in x))\n        loss = CrossEntropyLoss().forward(y_true=y, y_pred=y_pred)\n        return y_pred, loss\n\n    def evaluate(self, dateset: SimpleDataset) -&gt; tuple[float, int]:\n        # switch to eval mode\n        self.net.eval()\n        total_loss = 0.0\n        correct = 0\n        for x, y in dateset:\n            y_pred, loss = self.forward_once(x, y)\n            if y == 1:\n                correct += 1 if y_pred.data &gt; 0.5 else 0\n            else:\n                correct += 1 if y_pred.data &lt; 0.5 else 0\n            total_loss += loss.data\n\n        # switch back to train mode\n        self.net.train()\n\n        return total_loss, correct\n\n    def train(\n        self,\n        training_set: SimpleDataset,\n        test_set: SimpleDataset,\n        optimizer: Optimizer,\n        max_epochs: int = 500,\n    ) -&gt; Tuple[List[float], List[float], str]:\n        training_loss, testing_loss, test_correct = [], [], 0\n        for epoch in range(1, max_epochs + 1):\n            optimizer.zero_grad()\n\n            # Forward &amp; Backward\n            for x, y in training_set:\n                _, loss = self.forward_once(x, y)\n                (loss / len(training_set)).backward()\n\n            # Update parameters\n            optimizer.step()\n\n            # Evaluation\n            train_loss, train_correct = self.evaluate(training_set)\n            test_loss, test_correct = self.evaluate(test_set)\n\n            training_loss.append(train_loss)\n            testing_loss.append(test_loss)\n            if epoch % 10 == 0 or epoch == max_epochs:\n                print(\n                    f\"[Epoch {epoch}]Train Loss = {train_loss}, \"\n                    f\"right({train_correct})/total({len(training_set)}) = {train_correct / len(training_set)}\\n\"\n                    f\"[Epoch {epoch}]Test  Loss = {test_loss},  \"\n                    f\"right({test_correct})/total({len(test_set)}) = {test_correct / len(test_set)}\"\n                )\n        test_result = f\"right/total = {test_correct}/{len(test_set)}\"\n        return training_loss, testing_loss, test_result\n</code></pre>"},{"location":"quickstart/mlp/#_7","title":"\u5b9e\u9a8c\u5b8c\u6574\u4ee3\u7801","text":"\u672c\u793a\u4f8b\u7684\u5b8c\u6574\u4ee3\u7801: <code>example/mlp_binary.py</code> <pre><code>from pathlib import Path\nfrom typing import List, Tuple\n\nimport matplotlib.pyplot as plt\n\nimport toydl.dataset.simulation as simulation_dataset\n\nfrom toydl.core.optim import SGD, Momentum, Optimizer\nfrom toydl.core.scalar.scalar import Scalar\nfrom toydl.dataset.simple import SimpleDataset\nfrom toydl.loss.cross_entropy import CrossEntropyLoss\nfrom toydl.network.mlp import MLPBinaryClassifyNetFactory, MLPConfig\n\n\nclass MLPBinaryClassifyModel:\n    def __init__(self, mlp_config: MLPConfig):\n        self.net = MLPBinaryClassifyNetFactory(mlp_config)\n\n    def forward_once(self, x: list[float], y: int) -&gt; tuple[Scalar, Scalar]:\n        y_pred = self.net.forward(list(Scalar(v) for v in x))\n        loss = CrossEntropyLoss().forward(y_true=y, y_pred=y_pred)\n        return y_pred, loss\n\n    def evaluate(self, dateset: SimpleDataset) -&gt; tuple[float, int]:\n        # switch to eval mode\n        self.net.eval()\n        total_loss = 0.0\n        correct = 0\n        for x, y in dateset:\n            y_pred, loss = self.forward_once(x, y)\n            if y == 1:\n                correct += 1 if y_pred.data &gt; 0.5 else 0\n            else:\n                correct += 1 if y_pred.data &lt; 0.5 else 0\n            total_loss += loss.data\n\n        # switch back to train mode\n        self.net.train()\n\n        return total_loss, correct\n\n    def train(\n        self,\n        training_set: SimpleDataset,\n        test_set: SimpleDataset,\n        optimizer: Optimizer,\n        max_epochs: int = 500,\n    ) -&gt; Tuple[List[float], List[float], str]:\n        training_loss, testing_loss, test_correct = [], [], 0\n        for epoch in range(1, max_epochs + 1):\n            optimizer.zero_grad()\n\n            # Forward &amp; Backward\n            for x, y in training_set:\n                _, loss = self.forward_once(x, y)\n                (loss / len(training_set)).backward()\n\n            # Update parameters\n            optimizer.step()\n\n            # Evaluation\n            train_loss, train_correct = self.evaluate(training_set)\n            test_loss, test_correct = self.evaluate(test_set)\n\n            training_loss.append(train_loss)\n            testing_loss.append(test_loss)\n            if epoch % 10 == 0 or epoch == max_epochs:\n                print(\n                    f\"[Epoch {epoch}]Train Loss = {train_loss}, \"\n                    f\"right({train_correct})/total({len(training_set)}) = {train_correct / len(training_set)}\\n\"\n                    f\"[Epoch {epoch}]Test  Loss = {test_loss},  \"\n                    f\"right({test_correct})/total({len(test_set)}) = {test_correct / len(test_set)}\"\n                )\n        test_result = f\"right/total = {test_correct}/{len(test_set)}\"\n        return training_loss, testing_loss, test_result\n\n\n\ndef get_dataset(n: int = 100) -&gt; Tuple[SimpleDataset, SimpleDataset]:\n    data = simulation_dataset.diag(n)\n    training_set, test_set = data.train_test_split(train_proportion=0.7)\n\n    return training_set, test_set\n\n\n\n\ndef run_sgd(\n    mlp_config: MLPConfig,\n    training_set: SimpleDataset,\n    test_set: SimpleDataset,\n    learning_rate: float,\n    max_epochs: int = 500,\n):\n    mlp_model = MLPBinaryClassifyModel(mlp_config)\n\n    sgd_optimizer = SGD(mlp_model.net.parameters(), learning_rate)\n\n    training_loss, testing_loss, test_result = mlp_model.train(\n        training_set, test_set, sgd_optimizer, max_epochs=max_epochs\n    )\n    return training_loss, testing_loss, test_result\n\n\ndef run_momentum(\n    mlp_config: MLPConfig,\n    training_set: SimpleDataset,\n    test_set: SimpleDataset,\n    learning_rate: float,\n    max_epochs: int = 500,\n):\n    momentum = 0.5\n    mlp_model = MLPBinaryClassifyModel(mlp_config)\n\n    optimizer = Momentum(mlp_model.net.parameters(), learning_rate, momentum)\n\n    training_loss, testing_loss, test_result = mlp_model.train(\n        training_set, test_set, optimizer, max_epochs=max_epochs\n    )\n    return training_loss, testing_loss, test_result\n\n\ndef plot_multiple_optimizers(\n    optimizer_results: dict[str, tuple[list[float], list[float], str]],\n    plot_type: str = \"both\",  # \"train\", \"test\", or \"both\"\n    title: str = \"Optimizer Comparison\",\n    filename: str = \"optimizer_comparison.png\",\n    output_dir: Path | None = None,\n):\n    \"\"\"\n    Plot losses for multiple optimizers in one figure for comparison.\n    \"\"\"\n    plt.clf()\n    plt.figure(figsize=(10, 6))\n\n    colors = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\"]\n    markers = [\"o\", \"s\", \"^\", \"*\", \"x\", \"D\", \"v\"]\n\n    color_idx = 0\n    for optimizer_name, (training_loss, testing_loss, _) in optimizer_results.items():\n        color = colors[color_idx % len(colors)]\n        marker = markers[color_idx % len(markers)]\n\n        if plot_type in [\"train\", \"both\"]:\n            plt.plot(\n                training_loss,\n                f\"{color}-\",\n                label=f\"{optimizer_name} - Training\",\n                marker=marker,\n                markevery=max(1, len(training_loss) // 10),\n            )\n\n        if plot_type in [\"test\", \"both\"]:\n            plt.plot(\n                testing_loss,\n                f\"{color}--\",\n                label=f\"{optimizer_name} - Test\",\n                marker=marker,\n                markevery=max(1, len(testing_loss) // 10),\n            )\n\n        color_idx += 1\n\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True, linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n\n    # Handle output directory\n    if output_dir is not None:\n        output_dir.mkdir(exist_ok=True, parents=True)\n        save_path = output_dir / filename\n    else:\n        save_path = Path(f\"./{filename}\")\n\n    plt.savefig(save_path, dpi=300)\n    plt.show()\n\n    # Print the test results\n    print(\"\\nTest Results:\")\n    for optimizer_name, (_, _, test_result) in optimizer_results.items():\n        print(f\"{optimizer_name}: {test_result}\")\n\n\ndef run():\n    # Setup output directory using pathlib\n    current_file = Path(__file__)\n    image_dir = current_file.parent / \"images\"\n    image_dir.mkdir(exist_ok=True, parents=True)\n\n    n = 100\n    training_set, test_set = get_dataset(n)\n    training_set.plot(filename=str(image_dir / \"training_set.png\"))\n    mlp_config = MLPConfig(\n        in_size=2, out_size=1, hidden_layer_size=10, hidden_layer_num=2\n    )\n\n    learning_rate = 0.01\n    max_epochs = 500\n\n    sgd_training_loss, sgd_testing_loss, sgd_test_result = run_sgd(\n        mlp_config, training_set, test_set, learning_rate, max_epochs\n    )\n    momentum_training_loss, momentum_testing_loss, momentum_result = run_momentum(\n        mlp_config, training_set, test_set, learning_rate, max_epochs\n    )\n\n    # Plot comparison of optimizers\n    optimizer_results = {\n        \"SGD\": (sgd_training_loss, sgd_testing_loss, sgd_test_result),\n        \"Momentum\": (momentum_training_loss, momentum_testing_loss, momentum_result),\n    }\n    plot_multiple_optimizers(\n        optimizer_results,\n        title=\"Optimizer Loss Comparison\",\n        filename=\"optimizer_comparison.png\",\n        output_dir=image_dir,\n    )\n\n\nif __name__ == \"__main__\":\n    run()\n</code></pre>"}]}